{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4094520",
   "metadata": {},
   "source": [
    "# Semi_defect_finder\n",
    "ë°˜ë„ì²´ ê³µì •/ê³„ì¸¡ ë°ì´í„° ê¸°ë°˜ **ê²°í•¨ ì—¬ë¶€(ë¶ˆëŸ‰/ì •ìƒ)** ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶• ë…¸íŠ¸ë¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de0da8",
   "metadata": {},
   "source": [
    "[Phase 1] ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "VarianceThreshold â†’ ìƒê´€ê´€ê³„ ì œê±° â†’ ANOVA top-K\n",
    "\n",
    "[Phase 2] ë°ì´í„° ì „ëµë³„ ë¹„êµ  (í•µì‹¬ ì‹¤í—˜: Recall ê°’ ë¹„êµ)\n",
    "XGBoost, LightGBM, RF, LR ëª¨ë¸ë“¤ë¡œ\n",
    "\"Normal, Undersampled, SMOTE, PCA + ìœ„ 3ê°œ\" ê¸°ë²•ë“¤ë¡œ ì ìš© í›„ ë¹„êµë¶„ì„\n",
    "\n",
    "[Phase 3] ì´ìƒì¹˜ íƒì§€ (ë³„ë„ í”„ë ˆì„)\n",
    "EE / IF / LOF â†’ Recall ë¹„êµ\n",
    "\n",
    "[Phase 4] Top 3 ëª¨ë¸ GridSearchCV íŠœë‹\n",
    "\n",
    "[Phase 5] VotingClassifier ì•™ìƒë¸”\n",
    "Phase 2~4ì—ì„œ Recall ìƒìœ„ ëª¨ë¸ ì¡°í•©\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730442bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 1) Imports (preprocess & modeling)\n",
    "# =========================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, TimeSeriesSplit, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_recall_curve, roc_auc_score,\n",
    "    f1_score, precision_score, recall_score, confusion_matrix,\n",
    "    ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f26e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¹€íƒœí›ˆ\n",
    "import os\n",
    "os.chdir(\"/home/tae-hun-kim/ë°”íƒ•í™”ë©´/AIëª¨ë¸ê°œë°œ/Semiconductor_Quality_Prospect\")\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"exists:\", os.path.exists(\"SECOM/uci-secom.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58962f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ìµœì •ë¹ˆ\n",
    "# import os\n",
    "# os.chdir(r\"C:/Users/0726b/Semiconductor_Quality_Prospect\")  \n",
    "# print(\"cwd:\", os.getcwd())\n",
    "# print(\"exists:\", os.path.exists(\"SECOM/uci-secom.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 2) Load data & basic checks\n",
    "# =========================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸(.git) íƒìƒ‰\n",
    "root = Path.cwd()\n",
    "while root != root.parent and not (root / \".git\").exists():\n",
    "    root = root.parent\n",
    "\n",
    "csv_path = root / \"SECOM\" / \"uci-secom.csv\"\n",
    "print(\"Resolved csv_path:\", csv_path)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "target_col = \"Defect\"\n",
    "tool_col = \"Tool_Type\"\n",
    "\n",
    "print(\"Target col:\", target_col)\n",
    "print(\"Tool_Type col:\", tool_col)\n",
    "\n",
    "print(\"Missing ratio (top 10):\")\n",
    "print(df.isna().mean().sort_values(ascending=False).head(16))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d14361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 2-1) Drop Time column & correlation heatmap\n",
    "# =========================================\n",
    "\n",
    "df = df.drop(columns=[\"Time\"], errors=\"ignore\")\n",
    "print(\"Dropped Time:\", \"Time\" in df.columns)\n",
    "\n",
    "# Numeric-only correlation\n",
    "corr = df.select_dtypes(include=[\"number\"]).corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Feature Correlation (Time column removed)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf48a2",
   "metadata": {},
   "source": [
    "### Time ì œê±° â†’ ì „ë¶€ NaN/ë¶„ì‚° 0 ì»¬ëŸ¼ ì œê±° â†’ ìˆ«ìí˜• ìƒê´€ê³„ìˆ˜ ê³„ì‚° â†’ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b812f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df ê¸°ì¤€\n",
    "df = df.copy()\n",
    "\n",
    "# 1) ì „ë¶€ NaN ì»¬ëŸ¼\n",
    "all_nan_cols = df.columns[df.isna().all()].tolist()\n",
    "\n",
    "# 2) ë¶„ì‚° 0 ì»¬ëŸ¼ (ê°’ì´ ëª¨ë‘ ë™ì¼)\n",
    "zero_var_cols = df.columns[df.nunique(dropna=True) <= 1].tolist()\n",
    "\n",
    "print(\"all_nan:\", len(all_nan_cols))\n",
    "print(\"zero_var:\", len(zero_var_cols))\n",
    "\n",
    "# ì œê±°\n",
    "drop_cols = sorted(set(all_nan_cols + zero_var_cols))\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "# ë‹¤ì‹œ íˆíŠ¸ë§µ\n",
    "corr = df.select_dtypes(include=[\"number\"]).corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Feature Correlation (cleaned)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì ˆëŒ“ê°’ ìƒê´€ê³„ìˆ˜ 0.95 ì´ìƒì¸ ì»¬ëŸ¼ ì œê±°\n",
    "threshold = 0.95\n",
    "\n",
    "# ìˆ«ìí˜•ë§Œ ìƒê´€ê³„ìˆ˜\n",
    "corr = df.select_dtypes(include=[\"number\"]).corr().abs()\n",
    "\n",
    "# ìƒì‚¼ê°í–‰ë ¬ë§Œ ì‚¬ìš© (ì¤‘ë³µ ì œê±°)\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "\n",
    "# ì„ê³„ì¹˜ ì´ˆê³¼ ì»¬ëŸ¼ ì„ íƒ\n",
    "to_drop = [col for col in upper.columns if any(upper[col] >= threshold)]\n",
    "\n",
    "print(\"Drop count:\", len(to_drop))\n",
    "\n",
    "# í•„ìš”í•˜ë©´ ëª©ë¡ í™•ì¸\n",
    "print(to_drop)\n",
    "\n",
    "df = df.drop(columns=to_drop)\n",
    "\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# ì œê±° í›„ íˆíŠ¸ë§µ\n",
    "corr_after = df.select_dtypes(include=[\"number\"]).corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_after, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Feature Correlation (after dropping high-corr columns)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass/Failì„ 0/1ë¡œ ë³€í™˜ (-1: Pass -> 0, 1: Fail -> 1)\n",
    "df[\"Pass/Fail\"] = df[\"Pass/Fail\"].map({-1: 0, 1: 1})\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084466aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) ì»¬ëŸ¼ë³„ ê²°ì¸¡ë¥  ìƒìœ„ 30ê°œ\n",
    "missing_rate = df.isna().mean().sort_values(ascending=False)\n",
    "print(missing_rate.head(30))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "missing_rate.head(30).plot(kind=\"barh\")\n",
    "plt.title(\"Top 30 Missing Rates\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) í–‰ë³„ ê²°ì¸¡ë¥  ë¶„í¬\n",
    "row_missing = df.isna().mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(row_missing, bins=30, kde=True)\n",
    "plt.title(\"Row-wise Missing Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) ê²°ì¸¡ íŒ¨í„´ íˆíŠ¸ë§µ (ìƒ˜í”Œ 200í–‰)\n",
    "sample = df.sample(min(200, len(df)), random_state=42)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(sample.isna(), cbar=False)\n",
    "plt.title(\"Missing Pattern (sample)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) íƒ€ê¹ƒë³„ ê²°ì¸¡ë¥  ë¹„êµ (Pass/Fail ì»¬ëŸ¼ì´ ìˆì„ ë•Œ)\n",
    "if \"Pass/Fail\" in df.columns:\n",
    "    missing_by_target = df.groupby(\"Pass/Fail\").apply(lambda x: x.isna().mean())\n",
    "    print(missing_by_target.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ë¥  0.5 ì´ìƒ ì»¬ëŸ¼ ì œê±°\n",
    "threshold = 0.5\n",
    "missing_rate = df.isna().mean()\n",
    "drop_cols = missing_rate[missing_rate >= threshold].index.tolist()\n",
    "\n",
    "print(\"Drop count:\", len(drop_cols))\n",
    "print(drop_cols)\n",
    "\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "# í™•ì¸\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ì»¬ëŸ¼ë³„ ê²°ì¸¡ë¥  ìƒìœ„ 30ê°œ\n",
    "missing_rate = df.isna().mean().sort_values(ascending=False)\n",
    "print(missing_rate.head(30))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "missing_rate.head(30).plot(kind=\"barh\")\n",
    "plt.title(\"Top 30 Missing Rates\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) í–‰ë³„ ê²°ì¸¡ë¥  ë¶„í¬\n",
    "row_missing = df.isna().mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(row_missing, bins=30, kde=True)\n",
    "plt.title(\"Row-wise Missing Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) ê²°ì¸¡ íŒ¨í„´ íˆíŠ¸ë§µ (ìƒ˜í”Œ 200í–‰)\n",
    "sample = df.sample(min(200, len(df)), random_state=42)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(sample.isna(), cbar=False)\n",
    "plt.title(\"Missing Pattern (sample)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) íƒ€ê¹ƒë³„ ê²°ì¸¡ë¥  ë¹„êµ (Pass/Fail ì»¬ëŸ¼ì´ ìˆì„ ë•Œ)\n",
    "if \"Pass/Fail\" in df.columns:\n",
    "    missing_by_target = df.groupby(\"Pass/Fail\").apply(lambda x: x.isna().mean())\n",
    "    print(missing_by_target.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64093363",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Pass/Fail\"\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# KNN Imputer ì ìš©\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# ë‹¤ì‹œ DataFrameìœ¼ë¡œ\n",
    "X_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# KNNìœ¼ë¡œ ì±„ìš´ ê²°ê³¼ë¥¼ dfë¡œ í•©ì¹˜ê¸°\n",
    "df = df.copy()\n",
    "df[X.columns] = X_imputed  # Xì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ë§Œ\n",
    "\n",
    "# 3) ê²°ì¸¡ íŒ¨í„´ íˆíŠ¸ë§µ (ìƒ˜í”Œ 200í–‰)\n",
    "sample = df.sample(min(200, len(df)), random_state=42)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(sample.isna(), cbar=False)\n",
    "plt.title(\"Missing Pattern (after KNN Impute)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc976bc",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8197935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ì»¬ëŸ¼ íƒ€ì… ë¶„ë¦¬\n",
    "num_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "\n",
    "print(\"Numeric cols:\", len(num_cols))\n",
    "print(\"Categorical cols:\", len(cat_cols))\n",
    "\n",
    "# 2) ìˆ«ìí˜•: ë¶„ì‚° í° Top 10 (ë¶„í¬/ë°•ìŠ¤í”Œë¡¯ ì¶”ì²œ)\n",
    "num_var = df[num_cols].var().sort_values(ascending=False)\n",
    "print(\"\\nTop variance numeric cols:\")\n",
    "print(num_var.head(10))\n",
    "\n",
    "# 3) ìˆ«ìí˜•: ì™œë„ í° Top 10 (ë¡œê·¸ë³€í™˜ ì „/í›„ ë¹„êµ ì¶”ì²œ)\n",
    "num_skew = df[num_cols].skew().sort_values(ascending=False)\n",
    "print(\"\\nTop skew numeric cols:\")\n",
    "print(num_skew.head(10))\n",
    "\n",
    "# 4) ê²°ì¸¡ë¥  ë†’ì€ ì»¬ëŸ¼ Top 10 (ê²°ì¸¡ íŒ¨í„´/íƒ€ê¹ƒ ë¹„êµ ì¶”ì²œ)\n",
    "missing_rate = df.isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nTop missing rate cols:\")\n",
    "print(missing_rate.head(10))\n",
    "\n",
    "# 5) ìƒê´€ ë†’ì€ ì»¬ëŸ¼ ìŒ Top 10 (ì‚°ì ë„ ì¶”ì²œ)\n",
    "corr = df[num_cols].corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "high_corr_pairs = (\n",
    "    upper.stack()\n",
    "         .sort_values(ascending=False)\n",
    "         .head(10)\n",
    ")\n",
    "print(\"\\nTop correlated pairs:\")\n",
    "print(high_corr_pairs)\n",
    "\n",
    "# 6) ë²”ì£¼í˜•: ê°’ ê°œìˆ˜ ë§ì€ Top 10 (ë§‰ëŒ€ê·¸ë˜í”„/íŒŒì´ ì¶”ì²œ)\n",
    "if cat_cols:\n",
    "    cat_card = df[cat_cols].nunique().sort_values(ascending=False)\n",
    "    print(\"\\nTop cardinality categorical cols:\")\n",
    "    print(cat_card.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7113a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 4) Target Distribution (Pass/Fail Ratio) - Pastel Style\n",
    "# =========================================\n",
    "counts = df['Pass/Fail'].value_counts().sort_index()\n",
    "labels = ['Pass (0)', 'Fail (1)']\n",
    "colors = ['#6EC5E9', '#FF8A5B']  # íŒŒìŠ¤í…” ë¸”ë£¨/ì½”ë„\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar\n",
    "axes[0].bar(labels, counts.values, color=colors, edgecolor='white', linewidth=1.2)\n",
    "for i, v in enumerate(counts.values):\n",
    "    axes[0].text(i, v + 20, f'{v}', ha='center', fontweight='bold')\n",
    "axes[0].set_ylabel('Sample Count')\n",
    "axes[0].set_title('Pass/Fail Distribution')\n",
    "\n",
    "# Pie\n",
    "axes[1].pie(\n",
    "    counts.values,\n",
    "    labels=labels,\n",
    "    colors=colors,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    wedgeprops={'edgecolor': 'white', 'linewidth': 1.2},\n",
    "    textprops={'fontsize': 12}\n",
    ")\n",
    "axes[1].set_title('Pass/Fail Ratio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Pass: {counts.iloc[0]} ({counts.iloc[0]/len(df)*100:.1f}%)')\n",
    "print(f'Fail: {counts.iloc[1]} ({counts.iloc[1]/len(df)*100:.1f}%)')\n",
    "print(f'Imbalance ratio: 1:{counts.iloc[0]//counts.iloc[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552072c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Pass/Fail'])\n",
    "y = df['Pass/Fail']\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c00aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA + t-SNE í•œ ì¤„(1x2)ë¡œ ì¶œë ¥\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['Target'] = y.values\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "tsne_df = pd.DataFrame(data=X_tsne, columns=['tSNE1', 'tSNE2'])\n",
    "tsne_df['Target'] = y.values\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Target',\n",
    "                alpha=0.6, palette={-1: 'blue', 1: 'red'}, ax=axes[0])\n",
    "axes[0].set_title('PCA: 2D Projection of Sensor Data')\n",
    "\n",
    "sns.scatterplot(data=tsne_df, x='tSNE1', y='tSNE2', hue='Target',\n",
    "                alpha=0.6, palette={-1: 'blue', 1: 'red'}, ax=axes[1])\n",
    "axes[1].set_title('t-SNE: 2D Projection of Sensor Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815cb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier ìµœì í™”ëœ í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”\n",
    "\n",
    "imputer = SimpleImputer(strategy='median') # ì¤‘ì•™ê°’(Median) ëŒ€ì¹˜\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "selector = VarianceThreshold(threshold=0)\n",
    "X_clean = pd.DataFrame(selector.fit_transform(X_imputed), columns=X_imputed.columns[selector.get_support()])\n",
    "\n",
    "rf_best = RandomForestClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=10, \n",
    "    class_weight='balanced', \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_best.fit(X_clean, y)\n",
    "\n",
    "importances = pd.Series(rf_best.feature_importances_, index=X_clean.columns)\n",
    "top_features = importances.nlargest(10)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=top_features.values, y=top_features.index.astype(str), palette='viridis')\n",
    "\n",
    "plt.title('Feature Importance (Optimized: Median Impute + Balanced Weight + Depth Limit)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Sensor Feature ID')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ìƒìœ„ 10ê°œ ì¶œë ¥\n",
    "print(\"=== Top 10 Critical Sensors ===\")\n",
    "print(top_features.head(10))\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce044866",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "target = \"Pass/Fail\"\n",
    "top10 = [\"59\",\"103\",\"33\",\"510\",\"130\",\"247\",\"129\",\"205\",\"64\",\"21\"]\n",
    "palette_pf = {0: \"#6EC5E9\", 1: \"#FF8A5B\", \"0\": \"#6EC5E9\", \"1\": \"#FF8A5B\"}\n",
    "\n",
    "# =========================\n",
    "# 1) ë°•ìŠ¤í”Œë¡¯ (í‘œì¤€í™” + ë” ì˜ ë³´ì´ê²Œ)\n",
    "# =========================\n",
    "df_std = df.copy()\n",
    "df_std[top10] = (df_std[top10] - df_std[top10].mean()) / df_std[top10].std()\n",
    "\n",
    "df_melt = df_std.melt(id_vars=target, value_vars=top10, var_name=\"Feature\", value_name=\"Value\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.boxplot(\n",
    "    data=df_melt,\n",
    "    x=\"Feature\",\n",
    "    y=\"Value\",\n",
    "    hue=target,\n",
    "    palette=palette_pf,\n",
    "    linewidth=1.2,\n",
    "    fliersize=0,          # ì´ìƒì¹˜ ì  ìˆ¨ê¹€\n",
    "    whis=1.5\n",
    ")\n",
    "\n",
    "ymin, ymax = df_melt[\"Value\"].quantile([0.005, 0.995])\n",
    "plt.ylim(-5, 5)\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"Top 10 Features by Separation (Standardized, trimmed)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) íˆìŠ¤í† ê·¸ë¨ (ë²”ë¡€ ê·¸ë˜í”„ ë°–ìœ¼ë¡œ)\n",
    "# =========================\n",
    "g0 = df[df[target].astype(str) == \"0\"]\n",
    "g1 = df[df[target].astype(str) == \"1\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 6))\n",
    "for ax, col in zip(axes.flatten(), top10):\n",
    "    sns.histplot(g0[col], color=\"#6EC5E9\", label=\"Pass(0)\", kde=True,\n",
    "                 ax=ax, stat=\"density\", alpha=0.55)\n",
    "    sns.histplot(g1[col], color=\"#FF8A5B\", label=\"Fail(1)\", kde=True,\n",
    "                 ax=ax, stat=\"density\", alpha=0.55)\n",
    "    ax.set_title(str(col), fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper left\",\n",
    "           bbox_to_anchor=(0.001, 1.1), frameon=False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.88, 1])  # ì˜¤ë¥¸ìª½ ì—¬ë°± í™•ë³´\n",
    "plt.show()\n",
    "\n",
    "# í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ í™•ì¸ ìƒê´€ê³„ìˆ˜ë¥¼ ë³´ë˜ì§€ ì–´ë–¤ ì»¬ëŸ¼ì´ ì¤‘ìš”í•œì§€ ë¶„ì„ í•„ìš” ê¸°ì¤€ì´ ì—†ì–´ ì¼ë°˜ì ìœ¼ë¡œ ìƒê´€ê³„ìˆ˜ë¡œ í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f7b85",
   "metadata": {},
   "source": [
    " # Train / Test data ë¶„ë¦¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Pass/Fail\"\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target].astype(int)\n",
    "\n",
    "# 1) ë¨¼ì € ë¶„ë¦¬\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# 2) Trainìœ¼ë¡œë§Œ í”¼ì²˜ ì„ íƒ\n",
    "k = 20\n",
    "pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"anova\", SelectKBest(score_func=f_classif, k=k))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "selected_cols = X_train.columns[pipe.named_steps[\"anova\"].get_support()]\n",
    "\n",
    "# 3) Train/Valid/Test ëª¨ë‘ ë™ì¼ ì»¬ëŸ¼ë§Œ ìœ ì§€\n",
    "X_train = X_train[selected_cols]\n",
    "X_valid = X_valid[selected_cols]\n",
    "X_test  = X_test[selected_cols]\n",
    "\n",
    "print(\"Selected features:\", selected_cols.tolist())\n",
    "print(\"Train:\", X_train.shape)\n",
    "print(\"Valid:\", X_valid.shape)\n",
    "print(\"Test :\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Undersampling \n",
    "# =========================================\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# =========================================\n",
    "# Oversampling (SMOTE / Borderline-SMOTE)\n",
    "# =========================================\n",
    "# ì•„ë˜ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì£¼ì„ í•´ì œí•´ì„œ ì‚¬ìš©\n",
    "\n",
    "# --- SMOTE ---\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# --- Borderline-SMOTE ---\n",
    "# from imblearn.over_sampling import BorderlineSMOTE\n",
    "# bsm = BorderlineSMOTE(random_state=42, kind='borderline-1')\n",
    "# X_train, y_train = bsm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After Sampling:\", X_train.shape, y_train.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89459870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "947e8bbe",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
    "    ]),\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"model\", RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=12, random_state=42,\n",
    "            n_jobs=-1, class_weight=\"balanced\"\n",
    "        ))\n",
    "    ]),\n",
    "    \"HistGB\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"model\", HistGradientBoostingClassifier(\n",
    "            max_depth=8, learning_rate=0.05, random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \"XGBoost\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"model\", XGBClassifier(\n",
    "            n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "            random_state=42, n_jobs=-1, eval_metric=\"logloss\"\n",
    "        ))\n",
    "    ]),\n",
    "    \"LightGBM\": Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"model\", LGBMClassifier(\n",
    "            n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "            random_state=42, n_jobs=-1, verbose=-1\n",
    "        ))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(name, model, X_tr, y_tr, X_va, y_va):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    proba = model.predict_proba(X_va)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"ROC-AUC\": roc_auc_score(y_va, proba),\n",
    "        \"PR-AUC\": average_precision_score(y_va, proba),\n",
    "        \"Precision\": precision_score(y_va, pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_va, pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_va, pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    results.append(eval_model(name, model, X_train, y_train, X_valid, y_valid))\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"PR-AUC\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_best_threshold(model, X_tr, y_tr, X_va, y_va):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    proba = model.predict_proba(X_va)[:, 1]\n",
    "\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    best_f1 = (0, 0, 0)   # (thr, f1, recall)\n",
    "    best_rec = (0, 0, 0)  # (thr, recall, precision)\n",
    "\n",
    "    for thr in thresholds:\n",
    "        pred = (proba >= thr).astype(int)\n",
    "        f1 = f1_score(y_va, pred, zero_division=0)\n",
    "        rec = recall_score(y_va, pred, zero_division=0)\n",
    "        prec = precision_score(y_va, pred, zero_division=0)\n",
    "\n",
    "        if f1 > best_f1[1]:\n",
    "            best_f1 = (thr, f1, rec)\n",
    "        if rec > best_rec[1]:\n",
    "            best_rec = (thr, rec, prec)\n",
    "\n",
    "    return best_f1, best_rec\n",
    "\n",
    "# 5ê°œ ëª¨ë¸ ì „ì²´ ê²€í† \n",
    "for name, model in models.items():\n",
    "    best_f1, best_rec = find_best_threshold(model, X_train, y_train, X_valid, y_valid)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  [Best F1]    thr={best_f1[0]:.2f}, F1={best_f1[1]:.3f}, Recall={best_f1[2]:.3f}\")\n",
    "    print(f\"  [Best Recall] thr={best_rec[0]:.2f}, Recall={best_rec[1]:.3f}, Precision={best_rec[2]:.3f}\")\n",
    "    proba = model.predict_proba(X_valid)[:, 1]\n",
    "    print(f\"  PR-AUC: {average_precision_score(y_valid, proba):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d59f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8f79625",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ë³„ PR-AUC ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\n",
    "\n",
    "| ëª¨ë¸ (Algorithm) | Normal | Undersampling | SMOTE | Borderline-SMOTE |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| **Logistic Regression** | **0.279** | 0.220 | **0.236** | 0.215 |\n",
    "| **Random Forest** | 0.234 | 0.226 | 0.183 | 0.216 |\n",
    "| **HistGB** | 0.186 | 0.212 | 0.203 | 0.171 |\n",
    "| **XGBoost** | 0.221 | 0.172 | 0.159 | **0.252** |\n",
    "| **LightGBM** | 0.182 | **0.256** | 0.166 | 0.218 |\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ’¡ ê²°ê³¼ ë¶„ì„ ìš”ì•½\n",
    "1. **Best Model:** ì•„ë¬´ëŸ° ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•Šì€ **Normal ìƒíƒœì˜ Logistic Regression(0.279)**ì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n",
    "2. **Sampling íš¨ê³¼:** - **XGBoost**ëŠ” Borderline-SMOTE ì ìš© ì‹œ ì„±ëŠ¥ì´ í¬ê²Œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "   - **LightGBM**ì€ Undersamplingì—ì„œ ê°€ì¥ ì¢‹ì€ íš¨ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "3. **ì¢…í•© ì˜ê²¬:** ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬ê°€ í•­ìƒ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì¥í•˜ì§€ëŠ” ì•Šìœ¼ë¯€ë¡œ, í˜„ì¬ ë°ì´í„°ì…‹ì—ì„œëŠ” ëª¨ë¸ë³„ë¡œ ìµœì ì˜ ìƒ˜í”Œë§ ë°©ì‹ì´ ë‹¤ë¦„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a49653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import (\n",
    "#     roc_auc_score, average_precision_score,\n",
    "#     confusion_matrix, classification_report\n",
    "# )\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# best_name = \"RandomForest\"\n",
    "# best_thr = 0.16\n",
    "\n",
    "# model = models[best_name]\n",
    "# model.fit(X_train, y_train)\n",
    "# proba = model.predict_proba(X_test)[:, 1]\n",
    "# pred = (proba >= best_thr).astype(int)\n",
    "\n",
    "# print(\"ROC-AUC:\", roc_auc_score(y_test, proba))\n",
    "# print(\"PR-AUC :\", average_precision_score(y_test, proba))\n",
    "\n",
    "# # í¼ì„¼íŠ¸ë¡œ ì¶œë ¥\n",
    "# report = classification_report(y_test, pred, output_dict=True)\n",
    "# df_report = pd.DataFrame(report).T\n",
    "# for col in [\"precision\", \"recall\", \"f1-score\"]:\n",
    "#     df_report[col] = (df_report[col] * 100).round(1)\n",
    "\n",
    "# print(df_report[[\"precision\",\"recall\",\"f1-score\",\"support\"]])\n",
    "\n",
    "# # Confusion Matrix ì¶œë ¥/ì‹œê°í™”\n",
    "# cm = confusion_matrix(y_test, pred)\n",
    "# print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# plt.figure(figsize=(5, 4))\n",
    "# sns.heatmap(\n",
    "#     cm, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
    "#     xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"]\n",
    "# )\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.xlabel(\"Predictions\")\n",
    "# plt.ylabel(\"Actuals\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a834540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score\n",
    "\n",
    "# best_name = \"RandomForest\"\n",
    "# model = models[best_name]\n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "# proba = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# thresholds = np.linspace(0.01, 0.99, 99)\n",
    "# best = {\"thr\": None, \"f1\": -1, \"prec\": 0, \"rec\": 0}\n",
    "\n",
    "# for thr in thresholds:\n",
    "#     pred = (proba >= thr).astype(int)\n",
    "#     f1 = f1_score(y_valid, pred, zero_division=0)\n",
    "#     if f1 > best[\"f1\"]:\n",
    "#         best[\"thr\"] = thr\n",
    "#         best[\"f1\"] = f1\n",
    "#         best[\"prec\"] = precision_score(y_valid, pred, zero_division=0)\n",
    "#         best[\"rec\"] = recall_score(y_valid, pred, zero_division=0)\n",
    "\n",
    "# print(\"Best threshold by F1:\", best)\n",
    "# print(\"PR-AUC (model only):\", average_precision_score(y_valid, proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e83db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
